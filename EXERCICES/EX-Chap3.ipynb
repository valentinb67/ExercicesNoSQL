{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Exercise\"></a>\n",
    "\n",
    "### /!\\ J'ai eu quelques soucis en terme de stockage Github donc j'ai dû mettre une partie du script sous forme de commentaire pour limiter les outputs volumineux /!\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Exercise\"></a>\n",
    "\n",
    "### Exercises\n",
    "\n",
    "#### CRUD operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new db name Todo and a new collection named \"CRUD_exercise\" and do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database and Collection Created:  Todo CRUD_exercise\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['Todo']\n",
    "collection = db['CRUD_exercise']\n",
    "print(\"Database and Collection Created: \", db.name, collection.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 1**: Take the dict created in the TODO 4 in chapter I and save it in the collection \"CRUD_exercise\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('/Users/valen/Desktop/SAS/BC.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM tomatch3\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "data = [{'id': row[0], 'rdm_float': row[1]} for row in rows]\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted into MongoDB collection: CRUD_exercise\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['Todo']\n",
    "collection = db['CRUD_exercise']\n",
    "\n",
    "collection.insert_many(data)\n",
    "\n",
    "print(collection.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 2**: Insert 3 documents with key = x and values = 1, delete one of them. Which one is deleted first ? the most recent or oldest one ? increment the value of x to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted document IDs: [ObjectId('6630ee93e01c5a30842e1678'), ObjectId('6630ee93e01c5a30842e1679'), ObjectId('6630ee93e01c5a30842e167a')]\n"
     ]
    }
   ],
   "source": [
    "#Insert 3 documents with key = x and values = 1\n",
    "docs = [{'x': 1}, {'x': 1}, {'x': 1}]\n",
    "insert_result = collection.insert_many(docs)\n",
    "print(f\"Inserted document IDs: {insert_result.inserted_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted count: 1\n"
     ]
    }
   ],
   "source": [
    "#delete one of them\n",
    "delete_result = collection.delete_one({'x': 1})\n",
    "print(f\"Deleted count: {delete_result.deleted_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MongoDB ne va pas prendre en compte le plus récent ou plus ancien des documents mais supprimer le premier qu'il rencontre ayant la caractéristique demandé, dans notre cas, nous avons trois documents \"{'x': 1}\", donc il va en supprimer un au hasard s'il n'y a pas plus de spécification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated count: 1\n"
     ]
    }
   ],
   "source": [
    "#increment the value of x to 4\n",
    "update_result = collection.update_one({'x': 1}, {'$set': {'x': 4}})\n",
    "print(f\"Updated count: {update_result.modified_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 3**: Insert the dict created in the TODO 6 Chapter I in the example collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('/Users/valen/Desktop/SAS/BC.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM NN\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "dataNN = [{'id': row[0], 'rdm_float': row[1]} for row in rows]\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRUD_exercise\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['Todo']\n",
    "collection = db['CRUD_exercise']\n",
    "\n",
    "collection.insert_many(dataNN)\n",
    "\n",
    "print(collection.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 4**: Get documents where authors key exist in the collection \"CRUD_exercise\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = collection.find({'authors': {'$exists': True}})\n",
    "\n",
    "for doc in documents:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**TODO 5**: Change the documents where x = 4 to x = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult({'n': 0, 'nModified': 0, 'ok': 1.0, 'updatedExisting': False}, acknowledged=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.update_many({'x': 4}, {'$set': {'x': 1}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**TODO 6**: Find documents where author is not_mike and set author as real_mike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult({'n': 0, 'nModified': 0, 'ok': 1.0, 'updatedExisting': False}, acknowledged=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.update_many({'author': 'not_mike'}, {'$set': {'author': 'real_mike'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**TODO 7**: Delete documents where author is real_mike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteResult({'n': 0, 'ok': 1.0}, acknowledged=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.delete_many({'author': 'real_mike'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Managing DB\n",
    "\n",
    "**TODO 8**: create a collection named \"CRUD_exercise_benchmark\" with 500k observations, ids increment of 2 (sequence:0,2,4,6,...1M). Give a random np.array with a key named \"values\" and use the insert_many. Then create an index on the id and benchmark queries before and after indexing. Did the index help ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 500000 doc.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "start_time_no_index = time.time()\n",
    "doc = [{'id': i, 'values': np.random.rand(10).tolist()} for i in range(0, 1000000, 2)]\n",
    "\n",
    "insert_result = collection.insert_many(doc)\n",
    "end_time_no_index = time.time()\n",
    "print(f\"Inserted {len(insert_result.inserted_ids)} doc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "start_time_with_index = time.time()\n",
    "collection.create_index([('id', pymongo.ASCENDING)])\n",
    "end_time_with_index = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sans index: 13.77726674079895 secondes.\n",
      "Avec index: 0.003922224044799805 secondes.\n",
      "...soit un ratio de: 3512.615950398152\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sans index: {end_time_no_index - start_time_no_index} secondes.\")\n",
    "print(f\"Avec index: {end_time_with_index - start_time_with_index} secondes.\")\n",
    "print(f\"...soit un ratio de: {(end_time_no_index - start_time_no_index)/(end_time_with_index - start_time_with_index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oui, nous remarquons que la requête sans index mets beaucoup plus de temps que la requête avec index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 9**: create a random collection in a random db and put the new collection in the tutorial DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted into zwroewvwme.erqrbfzety\n",
      "Data copied to tutorial.erqrbfzety\n",
      "Database zwroewvwme deleted\n",
      "Collections in 'tutorial' database: ['image', 'benchmark_2', 'example', 'benchmark', 'arxiv_api', 'erqrbfzety', 'example_to_dump']\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "import string\n",
    "import random\n",
    "\n",
    "def random_string(length=10):\n",
    "    letters = string.ascii_lowercase\n",
    "    return ''.join(random.choice(letters) for i in range(length))\n",
    "\n",
    "client = MongoClient('localhost', 27017)\n",
    "\n",
    "random_db_name = random_string()\n",
    "random_col_name = random_string()\n",
    "\n",
    "random_db = client[random_db_name]\n",
    "random_collection = random_db[random_col_name]\n",
    "\n",
    "documents = [{'id': i, 'values': np.random.rand(10).tolist()} for i in range(10)]\n",
    "random_collection.insert_many(documents)\n",
    "print(f\"Data inserted into {random_db_name}.{random_col_name}\")\n",
    "\n",
    "tutorial_db = client['tutorial']\n",
    "tutorial_collection = tutorial_db[random_col_name]\n",
    "\n",
    "data_to_transfer = list(random_collection.find({}))\n",
    "tutorial_collection.insert_many(data_to_transfer)\n",
    "print(f\"Data copied to tutorial.{random_col_name}\")\n",
    "\n",
    "client.drop_database(random_db_name)\n",
    "print(f\"Database {random_db_name} deleted\")\n",
    "\n",
    "print(\"Collections in 'tutorial' database:\", tutorial_db.list_collection_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 10**: What is the difference between an inner join and an outer join ? Is the query seen during course an inner or outer join ? Play with the query to show all the joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real world problems\n",
    "\n",
    "**TODO 11**:  Use the oaipmh and api code get papers after January 2020 and for \"cs,math,econ\" categories. Insert them in MongoDB. Import only the first 200. How is it sorted ? How can you define your own sort()? Query papers to get papers after 2021, which have 3 authors and with domain \"cs\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 12**: Do the same as TODO 8 but with the connection to the cluster. Then check the metrics and take screenshot of opcounters, logical size and connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**TODO 13**: Download a random image and store it in a collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "img = \"https://i.redd.it/5hede1edzl561.jpg\"\n",
    "\n",
    "response = requests.get(img)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(\"LearnPythonMeme.jpg\", \"wb\") as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectId('66312ad1ea969c4b1a2070a8')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import base64\n",
    "from pymongo import MongoClient\n",
    "\n",
    "#MongoDB étant une base orientée document, nous aurons besoin de convertir la donnée binaire \n",
    "#en chaine de caractère ASCII grace à l'encodage en base64\n",
    "\n",
    "fp = \"/Users/valen/Documents/GitHub/ExercicesNoSQL/EXERCICES/LearnPythonMeme.jpg\"\n",
    "with open(fp, \"rb\") as LPM:\n",
    "    encoded_string = base64.b64encode(LPM.read())\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client[\"mydatabase\"]\n",
    "collection = db[\"images\"]\n",
    "\n",
    "image_document = {\n",
    "    \"image_name\": fp.split(\"/\")[-1],\n",
    "    \"image_data\": encoded_string\n",
    "}\n",
    "\n",
    "result = collection.insert_one(image_document)\n",
    "result.inserted_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**TODO 14**: Try to store a pandas dataframe in mongoDB (array with rownames, array with colnames and matrix with values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ObjectId('6631fa0aea969c4b1a7dbb3b'), ObjectId('6631fa0aea969c4b1a7dbb3c'), ObjectId('6631fa0aea969c4b1a7dbb3d')]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "data = {'Video game': ['Elden Ring', 'Metal Gear Solid: Sons of Liberty', 'Devil May Cry'],\n",
    "        'Year': [2022, 2001, 2001],\n",
    "        'Studio': ['FromSoftware ', 'Konami', 'Capcom']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['mydatabase']\n",
    "collection = db['VideoGames']\n",
    "\n",
    "\n",
    "records = df.to_dict('records')\n",
    "result = collection.insert_many(records)\n",
    "\n",
    "print(result.inserted_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video game</th>\n",
       "      <th>Year</th>\n",
       "      <th>Studio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Elden Ring</td>\n",
       "      <td>2022</td>\n",
       "      <td>FromSoftware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Metal Gear Solid: Sons of Liberty</td>\n",
       "      <td>2001</td>\n",
       "      <td>Konami</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Devil May Cry</td>\n",
       "      <td>2001</td>\n",
       "      <td>Capcom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Video game  Year         Studio\n",
       "0                         Elden Ring  2022  FromSoftware \n",
       "1  Metal Gear Solid: Sons of Liberty  2001         Konami\n",
       "2                      Devil May Cry  2001         Capcom"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 15**: Insert the movie_review.tsv data into mongodb. Then query it to find the number of review that are positive and negative review. Fetch the docs which have \"unexpected\" in their review, how many are they ? Think of a clever way to count the number of words in the review using MongoDB (hint: Transform the review text before the insert in MongoDB) and create a density of number of words per review.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 16**: Download a [sound sample](https://freesound.org/browse/). Try to store it in MongoDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import base64\n",
    "\n",
    "#Pour les mêmes raisons évoquée pour le stockage de l'image, nous devrons procéder \n",
    "#à un encodage en base64 pour stocker le fichier \".wav\" dans MongoDB\n",
    "wav_fp = '/Users/valen/Desktop/data/Chap3/734019__klankbeeld__storm-wind-in-trees-528-pm-160221_0857.wav'\n",
    "\n",
    "with wave.open(wav_fp, 'rb') as wav_file:\n",
    "    frames = wav_file.readframes(wav_file.getnframes())\n",
    "\n",
    "encoded_audio = base64.b64encode(frames).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsertOneResult(ObjectId('663137b4ea969c4b1a2070b1'), acknowledged=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['mydatabase']\n",
    "collection = db['audio_files']\n",
    "\n",
    "audio_document = {\n",
    "    \"file_name\": wav_fp.split(\"/\")[-1],\n",
    "    \"audio_data\": encoded_audio,\n",
    "    \"num_channels\": wav_file.getnchannels(),\n",
    "    \"sample_width\": wav_file.getsampwidth(),\n",
    "    \"frame_rate\": wav_file.getframerate(),\n",
    "    \"num_frames\": wav_file.getnframes()\n",
    "}\n",
    "\n",
    "collection.insert_one(audio_document)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 17**: Create a collection with 30M observation with a single key : \"year\" which is a random value between 2000-2020. Get documents with year = 2000. Does using an index helps ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps sans index: 4.274502515792847 secondes, Nombre: 4287432\n",
      "Temps avec index: 4.032424688339233 secondes, Nombre: 4287432\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import random\n",
    "from pymongo import MongoClient\n",
    "import time\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['large_data']\n",
    "collection = db['years']\n",
    "\n",
    "batch_size = 100000\n",
    "for _ in range(300):  # Insertion de 30 millions de documents\n",
    "    batch = [{\"year\": random.randint(2000, 2020)} for _ in range(batch_size)]\n",
    "    collection.insert_many(batch)\n",
    "\n",
    "if \"year_1\" not in collection.index_information(): \n",
    "    collection.create_index([(\"year\", pymongo.ASCENDING)])\n",
    "\n",
    "# Mesure de temps pour la query sans index\n",
    "start_time_no_index = time.time()\n",
    "count_no_index = collection.count_documents({\"year\": 2000})\n",
    "time_no_index = time.time() - start_time_no_index\n",
    "print(\"Temps sans index:\", time_no_index, \"secondes, Nombre:\", count_no_index)\n",
    "\n",
    "# Mesure de temps pour la query avec index (year)\n",
    "start_time_with_index = time.time()\n",
    "count_with_index = collection.count_documents({\"year\": 2000})\n",
    "time_with_index = time.time() - start_time_with_index\n",
    "print(\"Temps avec index:\", time_with_index, \"secondes, Nombre:\", count_with_index)\n",
    "\n",
    "client.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework 1\n",
    "\n",
    "pubmed_cleaned.zip is a file containing a metadatas sample from pubmed articles. Your goal is to **convert the json to a mongo DB** and answer the following questions **USING** mongodb querys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from bson import ObjectId\n",
    "from pymongo import MongoClient\n",
    "\n",
    "#Après essaies, j'ai remarqué que le fichier contenait un caractère spécial qui n'est pas\n",
    "#accepté par mongoDB dans le fichier mongoDB, il faut donc rempalcer \"$oid\" par un autre\n",
    "#nom ID\n",
    "\n",
    "def replace_oid(data):\n",
    "    \"\"\" Recursively replace '$oid' with ObjectId \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        if '$oid' in data:\n",
    "            return ObjectId(data['$oid'])\n",
    "        return {k: replace_oid(v) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [replace_oid(v) for v in data]\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "json_file_path = \"/Users/valen/Desktop/data/Chap3/pubmed_cleaned/pubmed_cleaned.json\"\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['mydatabase']  \n",
    "collection = db['pubmed_data'] \n",
    "\n",
    "with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)  \n",
    "\n",
    "    processed_data = replace_oid(data)\n",
    "\n",
    "    if isinstance(processed_data, list):\n",
    "        collection.insert_many(processed_data)\n",
    "    else:\n",
    "        collection.insert_one(processed_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1) Create an index, explain your choice of key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pmid_1'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Nous indexerons l'id \"pmid\"\n",
    "collection.create_index([(\"pmid\", 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Delete every paper that was published prior 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 articles ont été supprimés.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def convert_string_to_datetime(date_string):\n",
    "    year, month, day, hour, minute = map(int, date_string.split(\",\"))\n",
    "    return datetime(year, month, day, hour, minute)\n",
    "\n",
    "date_reference = datetime(2019, 1, 1)\n",
    "\n",
    "result = collection.delete_many({\"date\": {\"$lt\": date_reference}})\n",
    "\n",
    "print(f\"{result.deleted_count} articles ont été supprimés.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) How many paper have a single author ? Two authors ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de documents avec un seul auteur : 580\n",
      "Nombre de documents avec deux auteurs : 142\n"
     ]
    }
   ],
   "source": [
    "single_author_count = collection.aggregate([\n",
    "    {\"$match\": {\"authors\": {\"$exists\": True}}}, \n",
    "    {\"$project\": {\"num_authors\": {\"$size\": {\"$split\": [\"$authors\", \",\"]}}}},\n",
    "    {\"$match\": {\"num_authors\": 1}},\n",
    "    {\"$count\": \"count\"}\n",
    "])\n",
    "\n",
    "two_authors_count = collection.aggregate([\n",
    "    {\"$match\": {\"authors\": {\"$exists\": True}}}, \n",
    "    {\"$project\": {\"num_authors\": {\"$size\": {\"$split\": [\"$authors\", \",\"]}}}}, \n",
    "    {\"$match\": {\"num_authors\": 2}},\n",
    "    {\"$count\": \"count\"} \n",
    "])\n",
    "\n",
    "single_author_count = list(single_author_count)[0][\"count\"] if single_author_count.alive else 0\n",
    "two_authors_count = list(two_authors_count)[0][\"count\"] if two_authors_count.alive else 0\n",
    "\n",
    "print(f\"Nombre de documents avec un seul auteur : {single_author_count}\")\n",
    "print(f\"Nombre de documents avec deux auteurs : {two_authors_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) What's the last paper inserted in the db ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('60c753bac4b7559e040fe2b3'), 'pmid': 31226374, 'title': 'Identification of a novel UDP-glycosyltransferase gene from Rhodiola rosea and its expression during biotransformation of upstream precursors in callus culture.', 'ISSN': '1879-0003', 'abstract': ' \"Roseroot (Rhodiola rosea L.) is a medicinal plant with adaptogenic properties and several pharmaceutically important metabolites. In this study, a full length cDNA encoding a UDPG gene of roseroot was identified, cloned and characterized. Its ORF (1425bp) was transferred into E. coli, where the expression of the recombinant enzyme was confirmed. To monitor the enzyme activity, 3 precursors (tyramine, 4-hydroxyphenylpyruvate & tyrosol) of salidroside biosynthesis pathway were added to roseroot callus cultures and samples were harvested after 1, 6, 12, 24, 48 & 96h. Along with the controls (without precursor feeding), each sample was subjected to HPLC and qRT-PCR for phytochemical and relative UDP-glycosyltransferase gene expression analysis, respectively. The HPLC analysis showed that the salidroside content significantly increased; reaching 0.5% of the callus dry weight (26-fold higher than the control) after 96h when 2mM tyrosol was given to the media. The expression of the UDP-glycosyltransferase increased significantly being the highest at 12h after the feeding. The effect of tyramine and 4-hydroxyphenylpyruvate was not as pronounced as of tyrosol. Here, we introduce a R. rosea specific UDPG gene and its expression pattern after biotransformation of intermediate precursors in in vitro roseroot callus cultures.\", ', 'meshwords': 'Biotransformation \\nCulture Techniques \\nGene Expression Regulation\\nGlycosyltransferases\\nPhenylethyl Alcohol\\nPhylogeny \\nRhodiola\\nUridine Diphosphate', 'meshsubwords': 'genetics \\nmetabolism \\nanalogs & derivatives \\nmetabolism \\nenzymology \\ngenetics \\ngrowth & development \\nmetabolism ', 'authors': 'name ml Mirmazloum I, affil str Department of Plant Physiology and Plant Biochemistry, Szent Istvan University, Budapest, Hungary; Food Science Innovation Centre, Kaposvar University, Kaposvar, Hungary. Electronic address: Mirmazloum.Iman@kertk.szie.hu.\\nname ml Ladanyi M, affil str Department of Biometrics and Agricultural Informatics, Szent Istvan University, Budapest, Hungary.\\nname ml Beinrohr L, affil str Institute of Enzymology, Research Centre for Natural Sciences, Hungarian Academy of Sciences, Budapest, Hungary.\\nname ml Kiss-Baba E, affil str Department of Plant Physiology and Plant Biochemistry, Szent Istvan University, Budapest, Hungary.\\nname ml Kiss A, affil str Food Science Innovation Centre, Kaposvar University, Kaposvar, Hungary.\\nname ml Gyorgy Z, affil str Department of Genetics and Plant Breeding, Szent Istvan University, Budapest, Hungary.', 'source': 'journal: \"International journal of biological macromolecules\" ', 'grants': None, 'date': 'year 2019, month 6, day 22, hour 6, minute 0', 'date_received': 'year 2019, month 1, day 24', 'date_accepted': 'year 2019, month 6, day 12', 'date_medline': 'year 2020, month 1, day 17, hour 6, minute 0', 'doi': '10.1016/j.ijbiomac.2019.06.086', 'unix': 1561154400, 'unix_received': 1548284400, 'unix_accepted': 1560290400, 'unix_medline': 1579215600, 'team_size': 6, 'share_aff_captured': 1, 'is_eu': 1, 'inter_collab': 0, 'is_coronavirus_lower': 0, 'nb_country': 1, 'country_list': 'Hungary'}\n"
     ]
    }
   ],
   "source": [
    "last_paper = collection.find_one({}, sort=[('_id', -1)])\n",
    "\n",
    "print(last_paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Find articles with null meshwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_with_null_meshwords = collection.find({\"meshwords\": None})\n",
    "\n",
    "#for article in articles_with_null_meshwords:\n",
    "    #print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Choose a keyword you are interested in (machine learning, computer vision,...). Find the number of articles with the choosen keyword in their meshwords, abstract or title.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Nous allons choisir \"Green technology\"\n",
    "\n",
    "import re\n",
    "\n",
    "#Nous nous préoccupons du cas ou il s'agit de plusieurs technologies, en utilisant une boucle for\n",
    "keywords = [\"green technology\", \"green technologies\"]\n",
    "\n",
    "regex_patterns = [re.compile(keyword, re.IGNORECASE) for keyword in keywords]\n",
    "\n",
    "articles_count = collection.count_documents({\n",
    "    \"$or\": [\n",
    "        {\"meshwords\": {\"$in\": [pattern for pattern in regex_patterns]}},\n",
    "        {\"abstract\": {\"$in\": [pattern for pattern in regex_patterns]}},\n",
    "        {\"title\": {\"$in\": [pattern for pattern in regex_patterns]}}\n",
    "    ]\n",
    "})\n",
    "\n",
    "articles_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) What's the number of articles that have atleast one affiliation AND meshwords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74932"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_count = collection.count_documents({\n",
    "    \"authors\": {\"$regex\": \"affil\", \"$options\": \"i\"},\n",
    "    \"meshwords\": {\"$exists\": True, \"$ne\": None}\n",
    "})\n",
    "\n",
    "articles_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) How many articles have a publishing date after 2020 ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Find articles where there's atleast one affiliation from a choosen country (you decide which one).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2828"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_country = \"Netherlands\"\n",
    "\n",
    "articles_count = collection.count_documents({\n",
    "    \"authors\": {\"$regex\": chosen_country, \"$options\": \"i\"}})\n",
    "\n",
    "articles_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) Check for any duplicates. (hint: look at the doi or the pmid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doublons basés sur le champ DOI :\n",
      "DOI: 10.1093/cid/ciz518, Nbr :2\n",
      "DOI:  }, authors { names std { { name ml , Nbr :73\n",
      "DOI: , Nbr :5\n",
      "DOI: 10.1093/jas/skz192, Nbr :2\n",
      "DOI: , affil str , Nbr :40\n",
      "DOI:  } } }, from journal { title { iso-jta , Nbr :10\n",
      "DOI:  }, { name ml , Nbr :55\n",
      "DOI: 10.1093/jnci/djz062, Nbr :2\n",
      "DOI: 10.23876/j.krcp.19.006, Nbr :2\n"
     ]
    }
   ],
   "source": [
    "duplicates_by_doi = list(collection.aggregate([\n",
    "    {\"$match\": {\"doi\": {\"$ne\": None}}},\n",
    "    {\"$group\": {\"_id\": \"$doi\", \"count\": {\"$sum\": 1}}},\n",
    "    {\"$match\": {\"count\": {\"$gt\": 1}}}\n",
    "]))\n",
    "\n",
    "print(\"Doublons basés sur le champ DOI :\")\n",
    "for duplicate in duplicates_by_doi:\n",
    "    print(f\"DOI: {duplicate['_id']}, Nbr :{duplicate['count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11) Remove every articles where the abstract starts with an \"R\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collection.delete_many({\"abstract\": {\"$regex\": \"^R\", \"$options\": \"i\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) Return the list of papers (pmid) where there's atleast one affiliation per author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272\n",
      "Liste des PMIDs des articles répondant aux critères:\n",
      "[30647233, 30647235, 30647237, 30647238, 30647242, 30647243, 30647250, 30647261, 30647264, 30647267, 30647269, 30647271, 30647273, 30647280, 30647281, 30647287, 30647288, 30647291, 30647298, 30647299, 30647303, 30647305, 30647308, 30647309, 30647313, 30647314, 30647315, 30647318, 30647319, 30697754, 30697762, 30706834, 30706835, 30710987, 30710991, 30715029, 30715030, 30715036, 30717343, 30737981, 30759836, 30772820, 30790789, 30790793, 30790797, 30790801, 30790802, 30790803, 30790807, 30790809, 30790810, 30790812, 30790816, 30790817, 30790819, 30790822, 30790826, 30790831, 30790839, 30790845, 30790848, 30790852, 30790855, 30790861, 30790863, 30790864, 30790865, 30790870, 30790873, 30790874, 30790878, 30790880, 30790881, 30790884, 30790889, 30790892, 30790895, 30790898, 30790899, 30790902, 30790908, 30790910, 30790912, 30790918, 30790919, 30790921, 30790922, 30790926, 30790928, 30790930, 30790933, 30790934, 30790936, 30790938, 30790939, 30790940, 30790943, 30790945, 30790948, 30790949, 30790951, 30790952, 30790954, 30790955, 30790959, 30790960, 30790962, 30790965, 30790969, 30790972, 30790973, 30790980, 30790987, 30790989, 30790993, 30790994, 30790998, 30791000, 30791001, 30791002, 30791003, 30791006, 30791015, 30791019, 30791020, 30791023, 30791024, 30791025, 30791027, 30791030, 30791034, 30791594, 30818861, 30821248, 30823610, 30834855, 30834857, 30834863, 30834865, 30834866, 30834869, 30834870, 30834872, 30836690, 30847713, 30862128, 30864099, 30867221, 30867886, 30873732, 30875858, 30882863, 30885294, 30891152, 30893928, 30909536, 30916645, 30918588, 30920191, 30934980, 30934986, 30939235, 30943312, 30960603, 30962872, 30963288, 30965665, 30965669, 30968653, 30968654, 30968655, 30968662, 30968664, 30970637, 30970679, 30971833, 30972423, 30979095, 30985095, 30985097, 30987399, 30994456, 30995901, 31001868, 31003528, 31010023, 31012440, 31018619, 31020497, 31024297, 31025659, 31028203, 31037388, 31038287, 31040117, 31041361, 31041362, 31041363, 31041847, 31048411, 31048458, 31050855, 31055601, 31062339, 31062379, 31065308, 31065571, 31066036, 31083294, 31083368, 31084687, 31087034, 31087097, 31095332, 31099509, 31099510, 31099838, 31099987, 31102398, 31104297, 31106980, 31111079, 31111221, 31111488, 31111603, 31111894, 31115460, 31124259, 31125647, 31129622, 31131880, 31139259, 31141915, 31144100, 31144377, 31145797, 31147424, 31148603, 31151695, 31152443, 31155641, 31155851, 31157374, 31160625, 31161653, 31165163, 31165467, 31167847, 31177181, 31179504, 31182663, 31183853, 31185680, 31186299, 31186300, 31189585, 31189714, 31199102, 31199516, 31200574, 31207521, 31215648, 31216122, 31216166, 31217270, 31218355, 31218798, 31219103, 31219154, 31219665, 31223135, 31225597]\n"
     ]
    }
   ],
   "source": [
    "papers_with_one_affil_per_author_count = 0\n",
    "\n",
    "papers_with_one_affil_per_author = []\n",
    "\n",
    "articles = collection.find({\"$expr\": {\"$eq\": [{\"$size\": {\"$split\": [\"$authors\", \"affil\"]}}, \"$team_size\"]}})\n",
    "\n",
    "for article in articles:\n",
    "    papers_with_one_affil_per_author_count += 1\n",
    "    papers_with_one_affil_per_author.append(article[\"pmid\"])\n",
    "\n",
    "print(papers_with_one_affil_per_author_count)\n",
    "print(\"Liste des PMIDs des articles répondant aux critères:\")\n",
    "print(papers_with_one_affil_per_author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "13) Create 500 random samples of the dataset, compute a statistics that you are interested in and check how it behaves through the different samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "14) Sandbox exercise: think of a problematic and try to answer it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework 2\n",
    "\n",
    "authors.zip is a file containing a sample of authors that wrote a paper published on pubmed. Each doc as, at most, 5 keys. \"AND_ID\" is the disambiguated author id. \"pmid_list\" is the list of ids that the author published. \"more_info\" is a list of dict with each dict representing info for a given paper. \"oa04_affiliations\" is a list of dict with each dict representing affiliation info for a given paper. \"oa06_researcher_education\" is a list of dict with each dict containing information on the education of the researcher.\n",
    "\n",
    "Your goal is to **convert the json to a mongo DB** and answer the following questions **USING** mongodb querys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from bson import ObjectId\n",
    "from pymongo import MongoClient\n",
    "\n",
    "authors_fp = \"/Users/valen/Desktop/data/Chap3/authors.json\"\n",
    "\n",
    "#Nous avons utilisé la même fonction lors de l'exercice précédent\n",
    "def replace_oid(data):\n",
    "    \"\"\" Recursively replace '$oid' with ObjectId \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        if '_id' in data and '$oid' in data['_id']:\n",
    "            data['_id'] = ObjectId(data['_id']['$oid'])\n",
    "        for key, value in data.items():\n",
    "            data[key] = replace_oid(value)\n",
    "    elif isinstance(data, list):\n",
    "        for i, item in enumerate(data):\n",
    "            data[i] = replace_oid(item)\n",
    "    return data\n",
    "\n",
    "with open(authors_fp, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data = replace_oid(data)\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['mydatabase']\n",
    "collection = db['authors']\n",
    "\n",
    "#collection.insert_many(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Create an index, explain your choice of key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2) What is the average length of \"pmid_list\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_id': None, 'average_pmid_list_size': 7.040832449946997}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = [\n",
    "    {\"$match\": {\"pmid_list\": {\"$exists\": True, \"$type\": \"array\"}}},\n",
    "    {\"$project\": {\"pmid_list_size\": {\"$size\": \"$pmid_list\"}}},\n",
    "    {\"$group\": {\"_id\": None, \"average_pmid_list_size\": {\"$avg\": \"$pmid_list_size\"}}}\n",
    "]\n",
    "\n",
    "list(collection.aggregate(pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) How many distinct affiliations are there ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 318595 distinct affiliations.\n"
     ]
    }
   ],
   "source": [
    "pipeline = [\n",
    "    {\"$unwind\": \"$oa04_affiliations\"},\n",
    "    {\"$group\": {\"_id\": \"$oa04_affiliations.Affiliation\"}},\n",
    "    {\"$group\": {\"_id\": None, \"count\": {\"$sum\": 1}}}\n",
    "]\n",
    "\n",
    "result = list(collection.aggregate(pipeline))\n",
    "\n",
    "if result:\n",
    "    distinct_affiliations_count = result[0][\"count\"]\n",
    "    print(f\"There are {distinct_affiliations_count} distinct affiliations.\")\n",
    "else:\n",
    "    print(\"No affiliations found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Find authors with atleast one \"COM\" AffiliationType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    \"oa04_affiliations\": {\n",
    "        \"$elemMatch\": {\n",
    "            \"AffiliationType\": \"COM\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "authors_with_com_affiliation = collection.find(query)\n",
    "\n",
    "#for author in authors_with_com_affiliation:\n",
    "#    print(author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) How many authors switched the AffiliationType ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Find affiliation with the word \"China\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\"oa04_affiliations.Affiliation\": {\"$regex\": \"China\", \"$options\": \"i\"}}\n",
    "\n",
    "matching_documents = collection.find(query)\n",
    "\n",
    "#for document in matching_documents:\n",
    "    #affiliations = document.get(\"oa04_affiliations\", [])\n",
    "    #for affiliation in affiliations:\n",
    "        #if \"China\" in affiliation.get(\"Affiliation\", \"\"):\n",
    "            #print(affiliation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Get the pmids of papers published in 2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\"more_info.0.PubYear\": 2019}\n",
    "\n",
    "matching_documents = collection.find(query)\n",
    "\n",
    "pmids_2019 = []\n",
    "\n",
    "for document in matching_documents:\n",
    "    pmids = document.get(\"pmid_list\", [])\n",
    "    pmids_2019.extend(pmids)\n",
    "\n",
    "#print(pmids_2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Count the number of doc with \"oa06_researcher_education\" OR \"oa04_affiliations\" key and with the \"oa06_researcher_education\" AND \"oa04_affiliations\" .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) What's the average \"BeginYear\" of \"oa06_researcher_education\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) Count the distinct country of \"oa06_researcher_education\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11) Does the length of pmid_list and more_info always match ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) Does the length of pmid_list and \"oa04_affiliations\" always match ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13) Sandbox exercise: think of a problematic and try to answer it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
