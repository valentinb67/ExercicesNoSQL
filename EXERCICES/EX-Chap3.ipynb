{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Exercise\"></a>\n",
    "\n",
    "### Exercises\n",
    "\n",
    "#### CRUD operations\n",
    "\n",
    "Create a new db name Todo and a new collection named \"CRUD_exercise\" and do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database and Collection Created:  Todo CRUD_exercise\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['Todo']\n",
    "collection = db['CRUD_exercise']\n",
    "print(\"Database and Collection Created: \", db.name, collection.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 1**: Take the dict created in the TODO 4 in chapter I and save it in the collection \"CRUD_exercise\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('/Users/valen/Desktop/SAS/BC.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM tomatch3\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "data = [{'id': row[0], 'rdm_float': row[1]} for row in rows]\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted into MongoDB collection: CRUD_exercise\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['Todo']\n",
    "collection = db['CRUD_exercise']\n",
    "\n",
    "collection.insert_many(data)\n",
    "\n",
    "print(collection.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 2**: Insert 3 documents with key = x and values = 1, delete one of them. Which one is deleted first ? the most recent or oldest one ? increment the value of x to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted document IDs: [ObjectId('6630ee93e01c5a30842e1678'), ObjectId('6630ee93e01c5a30842e1679'), ObjectId('6630ee93e01c5a30842e167a')]\n"
     ]
    }
   ],
   "source": [
    "#Insert 3 documents with key = x and values = 1\n",
    "docs = [{'x': 1}, {'x': 1}, {'x': 1}]\n",
    "insert_result = collection.insert_many(docs)\n",
    "print(f\"Inserted document IDs: {insert_result.inserted_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted count: 1\n"
     ]
    }
   ],
   "source": [
    "#delete one of them\n",
    "delete_result = collection.delete_one({'x': 1})\n",
    "print(f\"Deleted count: {delete_result.deleted_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MongoDB ne va pas prendre en compte le plus récent ou plus ancien des documents mais supprimer le premier qu'il rencontre ayant la caractéristique demandé, dans notre cas, nous avons trois documents \"{'x': 1}\", donc il va en supprimer un au hasard s'il n'y a pas plus de spécification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated count: 1\n"
     ]
    }
   ],
   "source": [
    "#increment the value of x to 4\n",
    "update_result = collection.update_one({'x': 1}, {'$set': {'x': 4}})\n",
    "print(f\"Updated count: {update_result.modified_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 3**: Insert the dict created in the TODO 6 Chapter I in the example collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('/Users/valen/Desktop/SAS/BC.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM NN\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "dataNN = [{'id': row[0], 'rdm_float': row[1]} for row in rows]\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRUD_exercise\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['Todo']\n",
    "collection = db['CRUD_exercise']\n",
    "\n",
    "collection.insert_many(dataNN)\n",
    "\n",
    "print(collection.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 4**: Get documents where authors key exist in the collection \"CRUD_exercise\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = collection.find({'authors': {'$exists': True}})\n",
    "\n",
    "for doc in documents:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**TODO 5**: Change the documents where x = 4 to x = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult({'n': 0, 'nModified': 0, 'ok': 1.0, 'updatedExisting': False}, acknowledged=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.update_many({'x': 4}, {'$set': {'x': 1}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**TODO 6**: Find documents where author is not_mike and set author as real_mike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult({'n': 0, 'nModified': 0, 'ok': 1.0, 'updatedExisting': False}, acknowledged=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.update_many({'author': 'not_mike'}, {'$set': {'author': 'real_mike'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**TODO 7**: Delete documents where author is real_mike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteResult({'n': 0, 'ok': 1.0}, acknowledged=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.delete_many({'author': 'real_mike'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Managing DB\n",
    "\n",
    "**TODO 8**: create a collection named \"CRUD_exercise_benchmark\" with 500k observations, ids increment of 2 (sequence:0,2,4,6,...1M). Give a random np.array with a key named \"values\" and use the insert_many. Then create an index on the id and benchmark queries before and after indexing. Did the index help ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 500000 doc.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "start_time_no_index = time.time()\n",
    "doc = [{'id': i, 'values': np.random.rand(10).tolist()} for i in range(0, 1000000, 2)]\n",
    "\n",
    "insert_result = collection.insert_many(doc)\n",
    "end_time_no_index = time.time()\n",
    "print(f\"Inserted {len(insert_result.inserted_ids)} doc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "start_time_with_index = time.time()\n",
    "collection.create_index([('id', pymongo.ASCENDING)])\n",
    "end_time_with_index = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sans index: 13.77726674079895 secondes.\n",
      "Avec index: 0.003922224044799805 secondes.\n",
      "...soit un ratio de: 3512.615950398152\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sans index: {end_time_no_index - start_time_no_index} secondes.\")\n",
    "print(f\"Avec index: {end_time_with_index - start_time_with_index} secondes.\")\n",
    "print(f\"...soit un ratio de: {(end_time_no_index - start_time_no_index)/(end_time_with_index - start_time_with_index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oui, nous remarquons que la requête sans index mets beaucoup plus de temps que la requête avec index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 9**: create a random collection in a random db and put the new collection in the tutorial DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted into zwroewvwme.erqrbfzety\n",
      "Data copied to tutorial.erqrbfzety\n",
      "Database zwroewvwme deleted\n",
      "Collections in 'tutorial' database: ['image', 'benchmark_2', 'example', 'benchmark', 'arxiv_api', 'erqrbfzety', 'example_to_dump']\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "import string\n",
    "import random\n",
    "\n",
    "def random_string(length=10):\n",
    "    letters = string.ascii_lowercase\n",
    "    return ''.join(random.choice(letters) for i in range(length))\n",
    "\n",
    "client = MongoClient('localhost', 27017)\n",
    "\n",
    "random_db_name = random_string()\n",
    "random_col_name = random_string()\n",
    "\n",
    "random_db = client[random_db_name]\n",
    "random_collection = random_db[random_col_name]\n",
    "\n",
    "documents = [{'id': i, 'values': np.random.rand(10).tolist()} for i in range(10)]\n",
    "random_collection.insert_many(documents)\n",
    "print(f\"Data inserted into {random_db_name}.{random_col_name}\")\n",
    "\n",
    "tutorial_db = client['tutorial']\n",
    "tutorial_collection = tutorial_db[random_col_name]\n",
    "\n",
    "data_to_transfer = list(random_collection.find({}))\n",
    "tutorial_collection.insert_many(data_to_transfer)\n",
    "print(f\"Data copied to tutorial.{random_col_name}\")\n",
    "\n",
    "client.drop_database(random_db_name)\n",
    "print(f\"Database {random_db_name} deleted\")\n",
    "\n",
    "print(\"Collections in 'tutorial' database:\", tutorial_db.list_collection_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 10**: What is the difference between an inner join and an outer join ? Is the query seen during course an inner or outer join ? Play with the query to show all the joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real world problems\n",
    "\n",
    "**TODO 11**:  Use the oaipmh and api code get papers after January 2020 and for \"cs,math,econ\" categories. Insert them in MongoDB. Import only the first 200. How is it sorted ? How can you define your own sort()? Query papers to get papers after 2021, which have 3 authors and with domain \"cs\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 12**: Do the same as TODO 8 but with the connection to the cluster. Then check the metrics and take screenshot of opcounters, logical size and connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**TODO 13**: Download a random image and store it in a collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "img = \"https://i.redd.it/5hede1edzl561.jpg\"\n",
    "\n",
    "response = requests.get(img)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(\"LearnPythonMeme.jpg\", \"wb\") as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectId('66312ad1ea969c4b1a2070a8')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import base64\n",
    "from pymongo import MongoClient\n",
    "\n",
    "#MongoDB étant une base orientée document, nous aurons besoin de convertir la donnée binaire \n",
    "#en chaine de caractère ASCII grace à l'encodage en base64\n",
    "\n",
    "fp = \"/Users/valen/Documents/GitHub/ExercicesNoSQL/EXERCICES/LearnPythonMeme.jpg\"\n",
    "with open(fp, \"rb\") as LPM:\n",
    "    encoded_string = base64.b64encode(LPM.read())\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client[\"mydatabase\"]\n",
    "collection = db[\"images\"]\n",
    "\n",
    "image_document = {\n",
    "    \"image_name\": fp.split(\"/\")[-1],\n",
    "    \"image_data\": encoded_string\n",
    "}\n",
    "\n",
    "result = collection.insert_one(image_document)\n",
    "result.inserted_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**TODO 14**: Try to store a pandas dataframe in mongoDB (array with rownames, array with colnames and matrix with values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ObjectId('6631fa0aea969c4b1a7dbb3b'), ObjectId('6631fa0aea969c4b1a7dbb3c'), ObjectId('6631fa0aea969c4b1a7dbb3d')]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "data = {'Video game': ['Elden Ring', 'Metal Gear Solid: Sons of Liberty', 'Devil May Cry'],\n",
    "        'Year': [2022, 2001, 2001],\n",
    "        'Studio': ['FromSoftware ', 'Konami', 'Capcom']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['mydatabase']\n",
    "collection = db['VideoGames']\n",
    "\n",
    "\n",
    "records = df.to_dict('records')\n",
    "result = collection.insert_many(records)\n",
    "\n",
    "print(result.inserted_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video game</th>\n",
       "      <th>Year</th>\n",
       "      <th>Studio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Elden Ring</td>\n",
       "      <td>2022</td>\n",
       "      <td>FromSoftware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Metal Gear Solid: Sons of Liberty</td>\n",
       "      <td>2001</td>\n",
       "      <td>Konami</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Devil May Cry</td>\n",
       "      <td>2001</td>\n",
       "      <td>Capcom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Video game  Year         Studio\n",
       "0                         Elden Ring  2022  FromSoftware \n",
       "1  Metal Gear Solid: Sons of Liberty  2001         Konami\n",
       "2                      Devil May Cry  2001         Capcom"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 15**: Insert the movie_review.tsv data into mongodb. Then query it to find the number of review that are positive and negative review. Fetch the docs which have \"unexpected\" in their review, how many are they ? Think of a clever way to count the number of words in the review using MongoDB (hint: Transform the review text before the insert in MongoDB) and create a density of number of words per review.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 16**: Download a [sound sample](https://freesound.org/browse/). Try to store it in MongoDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import base64\n",
    "\n",
    "#Pour les mêmes raisons évoquée pour le stockage de l'image, nous devrons procéder \n",
    "#à un encodage en base64 pour stocker le fichier \".wav\" dans MongoDB\n",
    "wav_fp = '/Users/valen/Desktop/data/Chap3/734019__klankbeeld__storm-wind-in-trees-528-pm-160221_0857.wav'\n",
    "\n",
    "with wave.open(wav_fp, 'rb') as wav_file:\n",
    "    frames = wav_file.readframes(wav_file.getnframes())\n",
    "\n",
    "encoded_audio = base64.b64encode(frames).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsertOneResult(ObjectId('663137b4ea969c4b1a2070b1'), acknowledged=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['mydatabase']\n",
    "collection = db['audio_files']\n",
    "\n",
    "audio_document = {\n",
    "    \"file_name\": wav_fp.split(\"/\")[-1],\n",
    "    \"audio_data\": encoded_audio,\n",
    "    \"num_channels\": wav_file.getnchannels(),\n",
    "    \"sample_width\": wav_file.getsampwidth(),\n",
    "    \"frame_rate\": wav_file.getframerate(),\n",
    "    \"num_frames\": wav_file.getnframes()\n",
    "}\n",
    "\n",
    "collection.insert_one(audio_document)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 17**: Create a collection with 30M observation with a single key : \"year\" which is a random value between 2000-2020. Get documents with year = 2000. Does using an index helps ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps sans index: 4.274502515792847 secondes, Nombre: 4287432\n",
      "Temps avec index: 4.032424688339233 secondes, Nombre: 4287432\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import random\n",
    "from pymongo import MongoClient\n",
    "import time\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['large_data']\n",
    "collection = db['years']\n",
    "\n",
    "batch_size = 100000\n",
    "for _ in range(300):  # Insertion de 30 millions de documents\n",
    "    batch = [{\"year\": random.randint(2000, 2020)} for _ in range(batch_size)]\n",
    "    collection.insert_many(batch)\n",
    "\n",
    "if \"year_1\" not in collection.index_information(): \n",
    "    collection.create_index([(\"year\", pymongo.ASCENDING)])\n",
    "\n",
    "# Mesure de temps pour la query sans index\n",
    "start_time_no_index = time.time()\n",
    "count_no_index = collection.count_documents({\"year\": 2000})\n",
    "time_no_index = time.time() - start_time_no_index\n",
    "print(\"Temps sans index:\", time_no_index, \"secondes, Nombre:\", count_no_index)\n",
    "\n",
    "# Mesure de temps pour la query avec index (year)\n",
    "start_time_with_index = time.time()\n",
    "count_with_index = collection.count_documents({\"year\": 2000})\n",
    "time_with_index = time.time() - start_time_with_index\n",
    "print(\"Temps avec index:\", time_with_index, \"secondes, Nombre:\", count_with_index)\n",
    "\n",
    "client.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework 1\n",
    "\n",
    "pubmed_cleaned.zip is a file containing a metadatas sample from pubmed articles. Your goal is to **convert the json to a mongo DB** and answer the following questions **USING** mongodb querys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from bson import ObjectId\n",
    "from pymongo import MongoClient\n",
    "\n",
    "#Après essaies, j'ai remarqué que le fichier contenait un caractère spécial qui n'est pas\n",
    "#accepté par mongoDB dans le fichier mongoDB, il faut donc rempalcer \"$oid\" par un autre\n",
    "#nom ID\n",
    "\n",
    "def replace_oid(data):\n",
    "    \"\"\" Recursively replace '$oid' with ObjectId \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        if '$oid' in data:\n",
    "            return ObjectId(data['$oid'])\n",
    "        return {k: replace_oid(v) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [replace_oid(v) for v in data]\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "json_file_path = \"/Users/valen/Desktop/data/Chap3/pubmed_cleaned/pubmed_cleaned.json\"\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['mydatabase']  \n",
    "collection = db['pubmed_data'] \n",
    "\n",
    "with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)  \n",
    "\n",
    "    processed_data = replace_oid(data)\n",
    "\n",
    "    if isinstance(processed_data, list):\n",
    "        collection.insert_many(processed_data)\n",
    "    else:\n",
    "        collection.insert_one(processed_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1) Create an index, explain your choice of key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index created successfully\n"
     ]
    }
   ],
   "source": [
    "collection.create_index([(\"year\", 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index created successfully\n"
     ]
    }
   ],
   "source": [
    "collection.create_index([(\"author\", 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'year_1_author_1'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.create_index([(\"year\", 1), (\"author\", 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Delete every paper that was published prior 2019\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) How many paper have a single author ? Two authors ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) What's the last paper inserted in the db ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Find articles with null meshwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Choose a keyword you are interested in (machine learning, computer vision,...). Find the number of articles with the choosen keyword in their meshwords, abstract or title.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) What's the number of articles that have atleast one affiliation AND meshwords.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) How many articles have a publishing date after 2020 ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Find articles where there's atleast one affiliation from a choosen country (you decide which one).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) Check for any duplicates. (hint: look at the doi or the pmid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11) Remove every articles where the abstract starts with an \"R\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) Return the list of papers (pmid) where there's atleast one affiliation per author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "13) Create 500 random samples of the dataset, compute a statistics that you are interested in and check how it behaves through the different samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "14) Sandbox exercise: think of a problematic and try to answer it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework 2\n",
    "\n",
    "authors.zip is a file containing a sample of authors that wrote a paper published on pubmed. Each doc as, at most, 5 keys. \"AND_ID\" is the disambiguated author id. \"pmid_list\" is the list of ids that the author published. \"more_info\" is a list of dict with each dict representing info for a given paper. \"oa04_affiliations\" is a list of dict with each dict representing affiliation info for a given paper. \"oa06_researcher_education\" is a list of dict with each dict containing information on the education of the researcher.\n",
    "\n",
    "Your goal is to **convert the json to a mongo DB** and answer the following questions **USING** mongodb querys:\n",
    "\n",
    "1) Create an index, explain your choice of key.\n",
    "\n",
    "2) What is the average length of \"pmid_list\"\n",
    "\n",
    "3) How many distinct affiliations are there ?\n",
    "\n",
    "4) Find authors with atleast one \"COM\" AffiliationType\n",
    "\n",
    "5) How many authors switched the AffiliationType ?\n",
    "\n",
    "6) Find affiliation with the word \"China\" \n",
    "\n",
    "7) Get the pmids of papers published in 2019\n",
    "\n",
    "8) Count the number of doc with \"oa06_researcher_education\" OR \"oa04_affiliations\" key and with the \"oa06_researcher_education\" AND \"oa04_affiliations\" .\n",
    "\n",
    "9) What's the average \"BeginYear\" of \"oa06_researcher_education\".\n",
    "\n",
    "10) Count the distinct country of \"oa06_researcher_education\"\n",
    "\n",
    "11) Does the length of pmid_list and more_info always match ?\n",
    "\n",
    "12) Does the length of pmid_list and \"oa04_affiliations\" always match ?\n",
    "\n",
    "13) Sandbox exercise: think of a problematic and try to answer it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
