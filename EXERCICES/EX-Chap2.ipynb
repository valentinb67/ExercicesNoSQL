{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1**: Lorem Ipsum is just a random txt that devs use as a placeholder for multiple things (especially web developping) when you don't have the real text and just want to test your functionnality. Put a [Lorem Ipsum](https://www.lipsum.com/) of 3 paragraphs in a txt file using python, each paragraph delimited by two new line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lorem\n",
      "  Obtaining dependency information for lorem from https://files.pythonhosted.org/packages/90/7e/963834e2a400cefdf72af1e10a106c9e849cb4c149cc7b394cb1492effec/lorem-0.1.1-py3-none-any.whl.metadata\n",
      "  Downloading lorem-0.1.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Downloading lorem-0.1.1-py3-none-any.whl (5.0 kB)\n",
      "Installing collected packages: lorem\n",
      "Successfully installed lorem-0.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install lorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lorem  # Import the lorem package\n",
    "\n",
    "def create_lorem_ipsum_file(filename):\n",
    "    # Generate three paragraphs of lorem ipsum text\n",
    "    paragraphs = [lorem.paragraph() for _ in range(3)]\n",
    "    \n",
    "    # Combine the paragraphs, separating them with two new lines\n",
    "    lorem_ipsum_text = '\\n\\n\\n'.join(paragraphs)\n",
    "    \n",
    "    # Open the file in write mode and write the text\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(lorem_ipsum_text)\n",
    "\n",
    "# Specify the filename\n",
    "filename = \"lorem_ipsum.txt\"\n",
    "\n",
    "create_lorem_ipsum_file(filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2**: Update the txt file by removing the first paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_first_paragraph(filename):\n",
    "    # Open the file in read mode and read the content\n",
    "    with open(filename, 'r') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Split the content into paragraphs using three newlines as the delimiter\n",
    "    paragraphs = content.split('\\n\\n\\n')\n",
    "    \n",
    "    # Remove the first paragraph\n",
    "    if len(paragraphs) > 1:\n",
    "        remaining_paragraphs = '\\n\\n\\n'.join(paragraphs[1:])\n",
    "    else:\n",
    "        remaining_paragraphs = ''  # Handle the case where there's only one paragraph\n",
    "    \n",
    "    # Open the file in write mode and write the remaining paragraphs\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(remaining_paragraphs)\n",
    "\n",
    "# Specify the filename\n",
    "filename = \"lorem_ipsum.txt\"\n",
    "\n",
    "# Call the function to remove the first paragraph\n",
    "remove_first_paragraph(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3**: Create a dict from the paper of [lecun et al.](https://www.researchgate.net/publication/277411157_Deep_Learning) and [goodfellow et al.](https://arxiv.org/abs/1406.2661) with authors, title, affiliations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Deep Learning': {'authors': ['Yann LeCun', 'Y. Bengio', 'Geoffrey Hinton'], 'title': 'Deep Learning', 'affiliations': ['Université de Montréal', 'USA', 'USA']}, 'Generative Adversarial Networks': {'authors': ['Ian J. Goodfellow', 'Jean Pouget-Abadie', ' Mehdi Mirza'], 'title': 'Generative Adversarial Networks', 'affiliations': ['USA', 'University de Harvard', 'Na']}}\n"
     ]
    }
   ],
   "source": [
    "lecun_paper = {\"authors\" : [\"Yann LeCun\",\"Y. Bengio\",\"Geoffrey Hinton\"],\n",
    "         \"title\" : \"Deep Learning\",\n",
    "         \"affiliations\" : [\"Université de Montréal\", \"USA\", \"USA\"]}\n",
    "\n",
    "goodfellow_paper = {\"authors\" : [\"Ian J. Goodfellow\",\"Jean Pouget-Abadie\",\" Mehdi Mirza\"],\n",
    "         \"title\" : \"Generative Adversarial Networks\",\n",
    "         \"affiliations\" : [\"USA\",\"University de Harvard\",\"Na\"]}\n",
    "\n",
    "#paper = {\"authors\" : [\"Yann LeCun\",\"Y. Bengio\",\"Geoffrey Hinton\"], [\"Ian J. Goodfellow\",\"Jean Pouget-Abadie\",\" Mehdi Mirza\"],\n",
    "#         \"title\" : [\"Deep Learning\"], [\"Generative Adversarial Networks\"],\n",
    "#         \"affiliations\" : [\"Université de Montréal\"], [\"USA\",\"University de Harvard\",\"Na\"]}\n",
    "\n",
    "\n",
    "combined_papers = {\n",
    "    lecun_paper[\"title\"]: lecun_paper,\n",
    "    goodfellow_paper[\"title\"]: goodfellow_paper\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4**: Save the previously created dict in the JSON format and load it back.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Deep Learning': {'authors': ['Yann LeCun', 'Y. Bengio', 'Geoffrey Hinton'], 'title': 'Deep Learning', 'affiliations': ['Université de Montréal', 'USA', 'USA']}, 'Generative Adversarial Networks': {'authors': ['Ian J. Goodfellow', 'Jean Pouget-Abadie', ' Mehdi Mirza'], 'title': 'Generative Adversarial Networks', 'affiliations': ['USA', 'University de Harvard', 'Na']}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "file_path = 'data/Chap2/combined_papers.json'\n",
    "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "with open(file_path, 'w') as fp:\n",
    "    json.dump(combined_papers, fp)\n",
    "\n",
    "with open(file_path, 'r') as fp:\n",
    "    t= json.load(fp)\n",
    "\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5**: Save the previously created dict in the pickle format. Try to open manually (i.e with a text editor), is it human readable ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('combined_papers.pkl', 'wb') as file:\n",
    "    pickle.dump(combined_papers, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non, le document n'est pas human readable car il s'agit d'un formet destiné à l'efficacité plutôt qu'à la lisibilité et donc celui ci se présente comme une suite de caractère spéciaux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6**: Parse the xml_file2 in the same way as seen in the lecture: put infos in a dict and save it in a json file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "\n",
    "tree = ET.parse('/Users/valen/Documents/GitHub/NoSQL-master/data/Chap2/xml_file2.nxml')\n",
    "root = tree.getroot()\n",
    "\n",
    "data_dict = {}\n",
    "for element in root:\n",
    "    data_dict[element.tag] = element.text\n",
    "\n",
    "\n",
    "with open('xmlfile2.json', 'w') as json_file:\n",
    "    json.dump(data_dict, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7**: Download an image of your choice and save it in either jpg or png."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "img = \"https://m.media-amazon.com/images/I/51IkRtvi17L._AC_.jpg\"\n",
    "\n",
    "response = requests.get(img)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(\"MaleniaFanArt.jpg\", \"wb\") as file:\n",
    "        file.write(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8**: From the data/Chap2/data_world.json file, create a set of publisher type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = '/Users/valen/Documents/GitHub/NoSQL-master/data/Chap2/data_world.json'\n",
    "\n",
    "publisher_types = set()\n",
    "\n",
    "for item in file_path:\n",
    "    if 'publisher' in item and 'name' in item['publisher']:\n",
    "        # Add the publisher name to the set (which ensures uniqueness)\n",
    "        publisher_types.add(item['publisher']['name'])\n",
    "\n",
    "print(publisher_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9**: From the data/Chap2/data_world.json file, delete the key of your choice and save the new dict as data_world_cleaned.json.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has been cleaned and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Define the file paths\n",
    "input_file_path = '/Users/valen/Documents/GitHub/NoSQL-master/data/Chap2/data_world.json'\n",
    "output_file_path = 'data/Chap2/data_world_cleaned.json'\n",
    "\n",
    "# Make sure the directory exists (in case it doesn't)\n",
    "os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Step 1: Load the JSON file\n",
    "    with open(input_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Step 2: Modify the data by removing the 'description' key from each dataset\n",
    "    for dataset in data:\n",
    "        if 'description' in dataset:\n",
    "            del dataset['description']\n",
    "\n",
    "    # Step 3: Save the modified data to a new JSON file\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "    print(\"The file has been cleaned and saved successfully.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file {input_file_path} was not found.\")\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Error decoding JSON from the file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10**: From the data/Chap2/data_world.json file, create the co-occurence matrix between \"accessLevel\" and \"accrualPeriodicity\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accrualPeriodicity  R/P1D  R/P1M  R/P3M  R/PT1S  Unknown  irregular\n",
      "accessLevel                                                        \n",
      "public                  5      3      1       1       29       4961\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the JSON data\n",
    "def load_data(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Extract 'accessLevel' and 'accrualPeriodicity'\n",
    "def extract_fields(data):\n",
    "    occurrences = []\n",
    "    for item in data:\n",
    "        access_level = item.get('accessLevel', 'Unknown')\n",
    "        accrual_periodicity = item.get('accrualPeriodicity', 'Unknown')\n",
    "        occurrences.append((access_level, accrual_periodicity))\n",
    "    return occurrences\n",
    "\n",
    "# Create the co-occurrence matrix\n",
    "def create_cooccurrence_matrix(occurrences):\n",
    "    # Create a DataFrame from the occurrences\n",
    "    df = pd.DataFrame(occurrences, columns=['accessLevel', 'accrualPeriodicity'])\n",
    "    # Create a cross-tabulation which counts the occurrences of each combination\n",
    "    return pd.crosstab(df['accessLevel'], df['accrualPeriodicity'])\n",
    "\n",
    "# Main function to handle the workflow\n",
    "def main():\n",
    "    filepath = '/Users/valen/Documents/GitHub/NoSQL-master/data/Chap2/data_world.json'\n",
    "    data = load_data(filepath)\n",
    "    occurrences = extract_fields(data)\n",
    "    matrix = create_cooccurrence_matrix(occurrences)\n",
    "    print(matrix)\n",
    "\n",
    "# Execute the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
